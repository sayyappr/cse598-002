{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R5jAFRxhWtad"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/jxx123/simglucose.git\n",
        "!pip install d3rlpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "from simglucose.patient.t1dpatient import T1DPatient\n",
        "from simglucose.sensor.cgm import CGMSensor\n",
        "from simglucose.actuator.pump import InsulinPump\n",
        "from simglucose.simulation.env import T1DSimEnv\n",
        "from simglucose.simulation.scenario_gen import RandomScenario\n",
        "from simglucose.analysis.risk import risk_index\n",
        "\n",
        "# ----- Reward: RiskIndex-based -----\n",
        "\n",
        "REWARD_HORIZON = 24   # last 2 hours (24 * 5min = 120min)\n",
        "TMAX_STEPS     = 2500\n",
        "EARLY_PENALTY_ALPHA = 1.2\n",
        "EARLY_PENALTY_WORST_STEP = 100.0\n",
        "\n",
        "def full_risk_index_reward(bg_history, horizon=REWARD_HORIZON):\n",
        "    \"\"\"Return negative RiskIndex over last `horizon` BG values.\"\"\"\n",
        "    if len(bg_history) < horizon:\n",
        "        return 0.0\n",
        "    _, _, RI = risk_index(bg_history[-horizon:], horizon)\n",
        "    return -float(RI)\n",
        "\n",
        "# ----- Env construction -----\n",
        "\n",
        "def make_env(patient_id=\"adult#001\", seed=42):\n",
        "    patient = T1DPatient.withName(patient_id)\n",
        "    sensor  = CGMSensor.withName(\"Dexcom\")\n",
        "    pump    = InsulinPump.withName(\"Insulet\")\n",
        "\n",
        "    scen = RandomScenario(start_time=datetime.now(), seed=seed)\n",
        "    env  = T1DSimEnv(patient=patient, sensor=sensor, pump=pump, scenario=scen)\n",
        "    env.sample_time = 5  # minutes\n",
        "    return env\n",
        "\n",
        "# ----- State and evaluation metrics -----\n",
        "\n",
        "def obs_to_state(obs, info, t_minute):\n",
        "    \"\"\"\n",
        "    Continuous state vector for BCQ / TD3-BC.\n",
        "    You can enrich this later (slopes, TIR history, etc.).\n",
        "    \"\"\"\n",
        "    bg   = float(info[\"bg\"])\n",
        "    meal = float(info[\"meal\"])\n",
        "    iob  = float(info[\"patient_state\"][5])  # IOB index 5 from your tab RL code\n",
        "    time_of_day = float(t_minute % (24*60)) / (24*60.0)   # normalized [0,1]\n",
        "\n",
        "    return np.array([bg, meal, iob, time_of_day], dtype=np.float32)\n",
        "\n",
        "def compute_tir_tbr_cv(bg_series, low=70.0, high=180.0):\n",
        "    bg = np.asarray(bg_series, dtype=float)\n",
        "    valid = ~np.isnan(bg)\n",
        "    bg = bg[valid]\n",
        "    if len(bg) == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "\n",
        "    tir = np.mean((bg >= low) & (bg <= high)) * 100.0\n",
        "    tbr = np.mean(bg < low) * 100.0\n",
        "    cv  = np.std(bg) / max(np.mean(bg), 1e-6) * 100.0\n",
        "    return float(tir), float(tbr), float(cv)\n"
      ],
      "metadata": {
        "id": "qTxTo-9mWzsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simglucose.controller.base import Action\n",
        "from simglucose.patient.t1dpatient import T1DPatient\n",
        "from simglucose.sensor.cgm import CGMSensor\n",
        "from simglucose.actuator.pump import InsulinPump\n",
        "from simglucose.simulation.env import T1DSimEnv\n",
        "from simglucose.simulation.scenario_gen import RandomScenario\n",
        "from simglucose.analysis.risk import risk_index\n",
        "\n",
        "from d3rlpy.dataset import MDPDataset\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "REWARD_HORIZON = 24    # or 12, match your config\n",
        "TMAX_STEPS     = 2500\n",
        "\n",
        "def full_risk_index_reward(bg_history, horizon=REWARD_HORIZON):\n",
        "    if len(bg_history) < horizon:\n",
        "        return 0.0\n",
        "    _, _, RI = risk_index(bg_history[-horizon:], horizon)\n",
        "    return -float(RI)\n",
        "\n",
        "def make_env(patient_id=\"adult#001\", seed=42):\n",
        "    patient = T1DPatient.withName(patient_id)\n",
        "    sensor  = CGMSensor.withName(\"Dexcom\")\n",
        "    pump    = InsulinPump.withName(\"Insulet\")\n",
        "    scen    = RandomScenario(start_time=datetime.now(), seed=seed)\n",
        "    env     = T1DSimEnv(patient=patient, sensor=sensor, pump=pump, scenario=scen)\n",
        "    env.sample_time = 5\n",
        "    return env\n",
        "\n",
        "def obs_to_state(obs, info, t_minute):\n",
        "    bg   = float(info[\"bg\"])\n",
        "    meal = float(info[\"meal\"])\n",
        "    iob  = float(info[\"patient_state\"][5])\n",
        "    time_of_day = float(t_minute % (24 * 60)) / (24 * 60.0)\n",
        "    return np.array([bg, meal, iob, time_of_day], dtype=np.float32)\n",
        "\n",
        "def heuristic_policy(bg, meal, iob):\n",
        "    base = 0.05\n",
        "    meal_bonus = 0.01 * (meal > 0)\n",
        "    high_bg_bonus = 0.0005 * max(bg - 150, 0)\n",
        "    return float(base + meal_bonus + high_bg_bonus)\n",
        "\n",
        "def collect_offline_dataset(\n",
        "    n_episodes=80,\n",
        "    patient_id=\"adult#001\",\n",
        "    seed=42,\n",
        "    max_steps=TMAX_STEPS,\n",
        "    gamma=0.95\n",
        "):\n",
        "    rng = np.random.RandomState(seed)\n",
        "\n",
        "    observations      = []\n",
        "    actions           = []\n",
        "    rewards           = []\n",
        "    next_observations = []\n",
        "    terminals         = []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        env = make_env(patient_id=patient_id, seed=seed + ep)\n",
        "        step_result = env.reset()\n",
        "        obs = step_result.observation\n",
        "        info = step_result.info\n",
        "\n",
        "        bg_history = [info[\"bg\"]]\n",
        "        t_minute = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        print(f\"[Dataset] Episode {ep+1}/{n_episodes}\")\n",
        "\n",
        "        while not done and step < max_steps:\n",
        "            step += 1\n",
        "            t_minute += 5\n",
        "\n",
        "            bg   = float(info[\"bg\"])\n",
        "            meal = float(info[\"meal\"])\n",
        "            iob  = float(info[\"patient_state\"][5])\n",
        "\n",
        "            state = obs_to_state(obs, info, t_minute)\n",
        "\n",
        "            # === behavior policy ===\n",
        "            act_value = heuristic_policy(bg, meal, iob)\n",
        "            act_value += rng.normal(0.0, 0.01)\n",
        "            act_value = max(0.0, act_value)\n",
        "\n",
        "            # old: sim_action = env.action_space.sample()\n",
        "            # new: use Action directly\n",
        "            sim_action = Action(basal=0.0, bolus=act_value)\n",
        "\n",
        "            step_result = env.step(sim_action)\n",
        "            next_obs  = step_result.observation\n",
        "            done_env  = step_result.done\n",
        "            info      = step_result.info\n",
        "\n",
        "            new_bg = float(info[\"bg\"])\n",
        "            bg_history.append(new_bg)\n",
        "\n",
        "            # reward\n",
        "            r = full_risk_index_reward(bg_history, horizon=REWARD_HORIZON)\n",
        "\n",
        "            time_limit = (step >= max_steps)\n",
        "            terminal = done_env or time_limit\n",
        "\n",
        "            if done_env and not time_limit:\n",
        "                worst_step = 100.0\n",
        "                alpha      = 1.2\n",
        "                remaining  = max_steps - step\n",
        "                if remaining > 0:\n",
        "                    r += -alpha * worst_step * remaining\n",
        "\n",
        "            next_state = obs_to_state(next_obs, info, t_minute)\n",
        "\n",
        "            observations.append(state)\n",
        "            actions.append([act_value])\n",
        "            rewards.append(r)\n",
        "            next_observations.append(next_state)\n",
        "            terminals.append(terminal)\n",
        "\n",
        "            obs = next_obs\n",
        "            done = terminal\n",
        "\n",
        "    observations      = np.asarray(observations, dtype=np.float32)\n",
        "    actions           = np.asarray(actions, dtype=np.float32)\n",
        "    rewards           = np.asarray(rewards, dtype=np.float32)\n",
        "    next_observations = np.asarray(next_observations, dtype=np.float32)\n",
        "    terminals         = np.asarray(terminals, dtype=bool)\n",
        "\n",
        "    dataset = MDPDataset(\n",
        "        observations=observations,\n",
        "        actions=actions,\n",
        "        rewards=rewards,\n",
        "        terminals=terminals\n",
        "    )\n",
        "    return dataset\n",
        "\n",
        "\n",
        "dataset = collect_offline_dataset(\n",
        "    n_episodes= 5, #80,\n",
        "    patient_id=\"adult#001\",\n",
        "    seed=123\n",
        ")\n",
        "print(\"Dataset size:\", dataset.observations.shape)\n"
      ],
      "metadata": {
        "id": "0WC87YeQW75-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. BCQ\n"
      ],
      "metadata": {
        "id": "UpgljA6sXRIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from d3rlpy.algos import BCQ, BCQConfig\n",
        "\n",
        "bcq_config = BCQConfig(\n",
        "    gamma=0.95,\n",
        "    batch_size=256,\n",
        ")\n",
        "\n",
        "bcq = BCQ(bcq_config, device=\"cpu\", enable_ddp=False)\n",
        "\n",
        "# bcq.fit(\n",
        "#     dataset,\n",
        "#     n_epochs=50,\n",
        "#     n_steps_per_epoch=1000,\n",
        "# )\n",
        "\n",
        "bcq.fit(\n",
        "    dataset,\n",
        "    n_steps=2500,\n",
        "    n_steps_per_epoch=10_000,\n",
        "    save_interval=0,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "print(\"BCQ training done.\")\n"
      ],
      "metadata": {
        "id": "9GbgGyefXGfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. TD3\n"
      ],
      "metadata": {
        "id": "vHqvdbQ0XZCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from d3rlpy.algos import TD3PlusBC, TD3PlusBCConfig\n",
        "\n",
        "td3_config = TD3PlusBCConfig(\n",
        "    gamma=0.95,\n",
        "    alpha=2.5,         # behavior cloning regularization\n",
        "    batch_size=256,\n",
        ")\n",
        "\n",
        "td3_bc = td3_config.create(\n",
        "    device=\"cpu\",\n",
        "    enable_ddp=False,\n",
        ")\n",
        "\n",
        "td3_bc.fit(\n",
        "    dataset,\n",
        "    n_steps=2500,\n",
        "    n_steps_per_epoch=10_000,\n",
        "    save_interval=0,\n",
        ")\n",
        "\n",
        "print(\"TD3-BC training done.\")\n"
      ],
      "metadata": {
        "id": "ePDQzuCXXbHc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from simglucose.controller.base import Action\n",
        "import numpy as np\n",
        "\n",
        "def compute_tir_tbr_cv(bg_series, low=70.0, high=180.0):\n",
        "    bg = np.asarray(bg_series, dtype=float)\n",
        "    if len(bg) == 0:\n",
        "        return 0.0, 0.0, 0.0\n",
        "    tir = np.mean((bg >= low) & (bg <= high)) * 100.0\n",
        "    tbr = np.mean(bg < low) * 100.0\n",
        "    cv  = np.std(bg) / max(np.mean(bg), 1e-6) * 100.0\n",
        "    return float(tir), float(tbr), float(cv)\n",
        "\n",
        "\n",
        "def evaluate_policy(algo, n_episodes=10, patient_id=\"adult#001\", seed=999):\n",
        "    all_tir, all_tbr, all_cv = [], [], []\n",
        "\n",
        "    for ep in range(n_episodes):\n",
        "        env = make_env(patient_id=patient_id, seed=seed + ep)\n",
        "        step_result = env.reset()\n",
        "        obs  = step_result.observation\n",
        "        info = step_result.info\n",
        "\n",
        "        t_minute = 0\n",
        "        done = False\n",
        "        step = 0\n",
        "\n",
        "        bg_traj = []\n",
        "        bg_traj.append(float(info[\"bg\"]))\n",
        "\n",
        "        while not done and step < TMAX_STEPS:\n",
        "            step += 1\n",
        "            t_minute += 5\n",
        "\n",
        "            # your state encoding\n",
        "            state = obs_to_state(obs, info, t_minute)          # shape (4,)\n",
        "            state_vec = state.reshape(1, -1)                   # shape (1, 4)\n",
        "\n",
        "            # d3rlpy v2: predict -> array, extract scalar safely\n",
        "            action_arr = algo.predict(state_vec)               # shape (1, 1)\n",
        "            act_value = float(action_arr[0].item())            # scalar\n",
        "\n",
        "            # SimGlucose expects an Action, not env.action_space.sample()\n",
        "            sim_action = Action(basal=0.0, bolus=max(0.0, act_value))\n",
        "\n",
        "            step_result = env.step(sim_action)\n",
        "            obs  = step_result.observation\n",
        "            info = step_result.info\n",
        "\n",
        "            bg = float(info[\"bg\"])\n",
        "            bg_traj.append(bg)\n",
        "\n",
        "            done = step_result.done or (step >= TMAX_STEPS)\n",
        "\n",
        "        tir, tbr, cv = compute_tir_tbr_cv(bg_traj)\n",
        "        print(f\"[Eval] Episode {ep+1}/{n_episodes}  TIR={tir:.1f}  TBR={tbr:.1f}  CV={cv:.1f}\")\n",
        "        all_tir.append(tir)\n",
        "        all_tbr.append(tbr)\n",
        "        all_cv.append(cv)\n",
        "\n",
        "    return float(np.mean(all_tir)), float(np.mean(all_tbr)), float(np.mean(all_cv))\n",
        "\n",
        "\n",
        "bcq_tir, bcq_tbr, bcq_cv = evaluate_policy(bcq, n_episodes=10)\n",
        "td3_tir, td3_tbr, td3_cv = evaluate_policy(td3_bc, n_episodes=10)\n",
        "\n",
        "print(\"\\nBCQ    -> TIR={:.1f}  TBR={:.1f}  CV={:.1f}\".format(bcq_tir, bcq_tbr, bcq_cv))\n",
        "print(\"TD3-BC -> TIR={:.1f}  TBR={:.1f}  CV={:.1f}\".format(td3_tir, td3_tbr, td3_cv))\n"
      ],
      "metadata": {
        "id": "VSenqX1wXdaY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}