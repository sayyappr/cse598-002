{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5jAFRxhWtad"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/jxx123/simglucose.git\n",
    "!pip install d3rlpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTxTo-9mWzsB"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "from simglucose.patient.t1dpatient import T1DPatient\n",
    "from simglucose.sensor.cgm import CGMSensor\n",
    "from simglucose.actuator.pump import InsulinPump\n",
    "from simglucose.simulation.env import T1DSimEnv\n",
    "from simglucose.simulation.scenario_gen import RandomScenario\n",
    "from simglucose.analysis.risk import risk_index\n",
    "\n",
    "\n",
    "\n",
    "REWARD_HORIZON = 24   # last 2 hours (24 * 5min = 120min)\n",
    "TMAX_STEPS     = 2500\n",
    "EARLY_PENALTY_ALPHA = 1.2\n",
    "EARLY_PENALTY_WORST_STEP = 100.0\n",
    "\n",
    "def full_risk_index_reward(bg_history, horizon=REWARD_HORIZON):\n",
    "    \n",
    "    if len(bg_history) < horizon:\n",
    "        return 0.0\n",
    "    _, _, RI = risk_index(bg_history[-horizon:], horizon)\n",
    "    return -float(RI)\n",
    "\n",
    "\n",
    "\n",
    "def make_env(patient_id=\"adult#001\", seed=42):\n",
    "    patient = T1DPatient.withName(patient_id)\n",
    "    sensor  = CGMSensor.withName(\"Dexcom\")\n",
    "    pump    = InsulinPump.withName(\"Insulet\")\n",
    "\n",
    "    scen = RandomScenario(start_time=datetime.now(), seed=seed)\n",
    "    env  = T1DSimEnv(patient=patient, sensor=sensor, pump=pump, scenario=scen)\n",
    "    env.sample_time = 5  # minutes\n",
    "    return env\n",
    "\n",
    "\n",
    "\n",
    "def obs_to_state(obs, info, t_minute):\n",
    "    \n",
    "    bg   = float(info[\"bg\"])\n",
    "    meal = float(info[\"meal\"])\n",
    "    iob  = float(info[\"patient_state\"][5])  \n",
    "    time_of_day = float(t_minute % (24*60)) / (24*60.0)   \n",
    "\n",
    "    return np.array([bg, meal, iob, time_of_day], dtype=np.float32)\n",
    "\n",
    "def compute_tir_tbr_cv(bg_series, low=70.0, high=180.0):\n",
    "    bg = np.asarray(bg_series, dtype=float)\n",
    "    valid = ~np.isnan(bg)\n",
    "    bg = bg[valid]\n",
    "    if len(bg) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    tir = np.mean((bg >= low) & (bg <= high)) * 100.0\n",
    "    tbr = np.mean(bg < low) * 100.0\n",
    "    cv  = np.std(bg) / max(np.mean(bg), 1e-6) * 100.0\n",
    "    return float(tir), float(tbr), float(cv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WC87YeQW75-"
   },
   "outputs": [],
   "source": [
    "from simglucose.controller.base import Action\n",
    "from simglucose.patient.t1dpatient import T1DPatient\n",
    "from simglucose.sensor.cgm import CGMSensor\n",
    "from simglucose.actuator.pump import InsulinPump\n",
    "from simglucose.simulation.env import T1DSimEnv\n",
    "from simglucose.simulation.scenario_gen import RandomScenario\n",
    "from simglucose.analysis.risk import risk_index\n",
    "\n",
    "from d3rlpy.dataset import MDPDataset\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "REWARD_HORIZON = 24   \n",
    "TMAX_STEPS     = 2500\n",
    "\n",
    "def full_risk_index_reward(bg_history, horizon=REWARD_HORIZON):\n",
    "    if len(bg_history) < horizon:\n",
    "        return 0.0\n",
    "    _, _, RI = risk_index(bg_history[-horizon:], horizon)\n",
    "    return -float(RI)\n",
    "\n",
    "def make_env(patient_id=\"adult#001\", seed=42):\n",
    "    patient = T1DPatient.withName(patient_id)\n",
    "    sensor  = CGMSensor.withName(\"Dexcom\")\n",
    "    pump    = InsulinPump.withName(\"Insulet\")\n",
    "    scen    = RandomScenario(start_time=datetime.now(), seed=seed)\n",
    "    env     = T1DSimEnv(patient=patient, sensor=sensor, pump=pump, scenario=scen)\n",
    "    env.sample_time = 5\n",
    "    return env\n",
    "\n",
    "def obs_to_state(obs, info, t_minute):\n",
    "    bg   = float(info[\"bg\"])\n",
    "    meal = float(info[\"meal\"])\n",
    "    iob  = float(info[\"patient_state\"][5])\n",
    "    time_of_day = float(t_minute % (24 * 60)) / (24 * 60.0)\n",
    "    return np.array([bg, meal, iob, time_of_day], dtype=np.float32)\n",
    "\n",
    "def heuristic_policy(bg, meal, iob):\n",
    "    base = 0.05\n",
    "    meal_bonus = 0.01 * (meal > 0)\n",
    "    high_bg_bonus = 0.0005 * max(bg - 150, 0)\n",
    "    return float(base + meal_bonus + high_bg_bonus)\n",
    "\n",
    "def collect_offline_dataset(\n",
    "    n_episodes=80,\n",
    "    patient_id=\"adult#001\",\n",
    "    seed=42,\n",
    "    max_steps=TMAX_STEPS,\n",
    "    gamma=0.95\n",
    "):\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    observations      = []\n",
    "    actions           = []\n",
    "    rewards           = []\n",
    "    next_observations = []\n",
    "    terminals         = []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        env = make_env(patient_id=patient_id, seed=seed + ep)\n",
    "        step_result = env.reset()\n",
    "        obs = step_result.observation\n",
    "        info = step_result.info\n",
    "\n",
    "        bg_history = [info[\"bg\"]]\n",
    "        t_minute = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        print(f\"[Dataset] Episode {ep+1}/{n_episodes}\")\n",
    "\n",
    "        while not done and step < max_steps:\n",
    "            step += 1\n",
    "            t_minute += 5\n",
    "\n",
    "            bg   = float(info[\"bg\"])\n",
    "            meal = float(info[\"meal\"])\n",
    "            iob  = float(info[\"patient_state\"][5])\n",
    "\n",
    "            state = obs_to_state(obs, info, t_minute)\n",
    "\n",
    "       \n",
    "            act_value = heuristic_policy(bg, meal, iob)\n",
    "            act_value += rng.normal(0.0, 0.01)\n",
    "            act_value = max(0.0, act_value)\n",
    "\n",
    "            \n",
    "            sim_action = Action(basal=0.0, bolus=act_value)\n",
    "\n",
    "            step_result = env.step(sim_action)\n",
    "            next_obs  = step_result.observation\n",
    "            done_env  = step_result.done\n",
    "            info      = step_result.info\n",
    "\n",
    "            new_bg = float(info[\"bg\"])\n",
    "            bg_history.append(new_bg)\n",
    "\n",
    "            # reward\n",
    "            r = full_risk_index_reward(bg_history, horizon=REWARD_HORIZON)\n",
    "\n",
    "            time_limit = (step >= max_steps)\n",
    "            terminal = done_env or time_limit\n",
    "\n",
    "            if done_env and not time_limit:\n",
    "                worst_step = 100.0\n",
    "                alpha      = 1.2\n",
    "                remaining  = max_steps - step\n",
    "                if remaining > 0:\n",
    "                    r += -alpha * worst_step * remaining\n",
    "\n",
    "            next_state = obs_to_state(next_obs, info, t_minute)\n",
    "\n",
    "            observations.append(state)\n",
    "            actions.append([act_value])\n",
    "            rewards.append(r)\n",
    "            next_observations.append(next_state)\n",
    "            terminals.append(terminal)\n",
    "\n",
    "            obs = next_obs\n",
    "            done = terminal\n",
    "\n",
    "    observations      = np.asarray(observations, dtype=np.float32)\n",
    "    actions           = np.asarray(actions, dtype=np.float32)\n",
    "    rewards           = np.asarray(rewards, dtype=np.float32)\n",
    "    next_observations = np.asarray(next_observations, dtype=np.float32)\n",
    "    terminals         = np.asarray(terminals, dtype=bool)\n",
    "\n",
    "    dataset = MDPDataset(\n",
    "        observations=observations,\n",
    "        actions=actions,\n",
    "        rewards=rewards,\n",
    "        terminals=terminals\n",
    "    )\n",
    "    return dataset\n",
    "\n",
    "\n",
    "dataset = collect_offline_dataset(\n",
    "    n_episodes= 5, #80,\n",
    "    patient_id=\"adult#001\",\n",
    "    seed=123\n",
    ")\n",
    "print(\"Dataset size:\", dataset.observations.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpgljA6sXRIm"
   },
   "source": [
    "1. BCQ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9GbgGyefXGfM"
   },
   "outputs": [],
   "source": [
    "from d3rlpy.algos import BCQ, BCQConfig\n",
    "\n",
    "bcq_config = BCQConfig(\n",
    "    gamma=0.95,\n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "bcq = BCQ(bcq_config, device=\"cpu\", enable_ddp=False)\n",
    "\n",
    "\n",
    "\n",
    "bcq.fit(\n",
    "    dataset,\n",
    "    n_steps=2500,\n",
    "    n_steps_per_epoch=10_000,\n",
    "    save_interval=0,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "print(\"BCQ training done.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHqvdbQ0XZCs"
   },
   "source": [
    "2. TD3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePDQzuCXXbHc"
   },
   "outputs": [],
   "source": [
    "from d3rlpy.algos import TD3PlusBC, TD3PlusBCConfig\n",
    "\n",
    "td3_config = TD3PlusBCConfig(\n",
    "    gamma=0.95,\n",
    "    alpha=2.5,         \n",
    "    batch_size=256,\n",
    ")\n",
    "\n",
    "td3_bc = td3_config.create(\n",
    "    device=\"cpu\",\n",
    "    enable_ddp=False,\n",
    ")\n",
    "\n",
    "td3_bc.fit(\n",
    "    dataset,\n",
    "    n_steps=2500,\n",
    "    n_steps_per_epoch=10_000,\n",
    "    save_interval=0,\n",
    ")\n",
    "\n",
    "print(\"TD3-BC training done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSenqX1wXdaY"
   },
   "outputs": [],
   "source": [
    "from simglucose.controller.base import Action\n",
    "import numpy as np\n",
    "\n",
    "def compute_tir_tbr_cv(bg_series, low=70.0, high=180.0):\n",
    "    bg = np.asarray(bg_series, dtype=float)\n",
    "    if len(bg) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "    tir = np.mean((bg >= low) & (bg <= high)) * 100.0\n",
    "    tbr = np.mean(bg < low) * 100.0\n",
    "    cv  = np.std(bg) / max(np.mean(bg), 1e-6) * 100.0\n",
    "    return float(tir), float(tbr), float(cv)\n",
    "\n",
    "\n",
    "def evaluate_policy(algo, n_episodes=10, patient_id=\"adult#001\", seed=999):\n",
    "    all_tir, all_tbr, all_cv = [], [], []\n",
    "\n",
    "    for ep in range(n_episodes):\n",
    "        env = make_env(patient_id=patient_id, seed=seed + ep)\n",
    "        step_result = env.reset()\n",
    "        obs  = step_result.observation\n",
    "        info = step_result.info\n",
    "\n",
    "        t_minute = 0\n",
    "        done = False\n",
    "        step = 0\n",
    "\n",
    "        bg_traj = []\n",
    "        bg_traj.append(float(info[\"bg\"]))\n",
    "\n",
    "        while not done and step < TMAX_STEPS:\n",
    "            step += 1\n",
    "            t_minute += 5\n",
    "\n",
    "            \n",
    "            state = obs_to_state(obs, info, t_minute)          \n",
    "            state_vec = state.reshape(1, -1)                   \n",
    "\n",
    "            \n",
    "            action_arr = algo.predict(state_vec)               \n",
    "            act_value = float(action_arr[0].item())            \n",
    "\n",
    "            \n",
    "            sim_action = Action(basal=0.0, bolus=max(0.0, act_value))\n",
    "\n",
    "            step_result = env.step(sim_action)\n",
    "            obs  = step_result.observation\n",
    "            info = step_result.info\n",
    "\n",
    "            bg = float(info[\"bg\"])\n",
    "            bg_traj.append(bg)\n",
    "\n",
    "            done = step_result.done or (step >= TMAX_STEPS)\n",
    "\n",
    "        tir, tbr, cv = compute_tir_tbr_cv(bg_traj)\n",
    "        print(f\"[Eval] Episode {ep+1}/{n_episodes}  TIR={tir:.1f}  TBR={tbr:.1f}  CV={cv:.1f}\")\n",
    "        all_tir.append(tir)\n",
    "        all_tbr.append(tbr)\n",
    "        all_cv.append(cv)\n",
    "\n",
    "    return float(np.mean(all_tir)), float(np.mean(all_tbr)), float(np.mean(all_cv))\n",
    "\n",
    "\n",
    "bcq_tir, bcq_tbr, bcq_cv = evaluate_policy(bcq, n_episodes=10)\n",
    "td3_tir, td3_tbr, td3_cv = evaluate_policy(td3_bc, n_episodes=10)\n",
    "\n",
    "print(\"\\nBCQ    -> TIR={:.1f}  TBR={:.1f}  CV={:.1f}\".format(bcq_tir, bcq_tbr, bcq_cv))\n",
    "print(\"TD3-BC -> TIR={:.1f}  TBR={:.1f}  CV={:.1f}\".format(td3_tir, td3_tbr, td3_cv))\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
